
# Чеклист по лекции:
1) Какие типы падений есть в streaming? Чем отличается отказ во время обработки  от отказа отправки ответа о результатах?
2) Подходы для обработки потоковых данных? Обработка батчами против обработки на лету?
3) Что такое лямбда архитектура и почему ее удобно реализовывать в спрак?
4) Как спарк представляет поток данных в таблицы? В чем разница Dstream от Structured dataframe? 
5) Как работает Dstream в спарк? Как выполняется операция Map для стриминга?
6) В Structured Streaming dataframe чем отличается временное окно для триггера  от временно окна обработки?
7) Что такое Kafka? Что такое topic в  kafka?
8) По каким портам подключаются обычно jupyter, spark ui?
9) Что такое Streaming Spark context? За что отвечает флаг StopGracefully?
10) Что такое zookeeper?


Практика:
Друзья, нашла оптимальный вариант для практики с Docker, Spark Streaming, Kafka  - там еще плюс Cassandra  мы говорили про No-SQL - Cassandra как раз очень хороший пример
Вот ссылка про нее https://ru.wikipedia.org/wiki/Apache_Cassandra

Практика:
Прикрепляю ссылку на гитхаб с проектом - там ребята написали несколько важных и удобных вещей:
1) Докер с установленным необходимым окружением, который вам поднимет ноутбук
2) Два ноутбука, которые непосредственно организуют пересылку через spark streaming + kafka и складывают результат в кассандру

Что нужно:
Запустить проект, запустить ноутбуки. 
https://github.com/Yannael/kafka-sparkstreaming-cassandra
Про интеграцию с kafka у spark-a хорошие сниппеты есть
Вот ссылка: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html

Какие есть трудности:
1) Нужен установленный докер - на этом шаге поможет
https://docs.docker.com/get-docker/
2) Могут быть неприятные ошибки - тут прикрепляю небольшое видео, как в идеальном мире будет работать этот проект. Звука нет - так и должно быть. + несколько полезных команд docker-a. Если не получится все-таки запустить проект - то в видео вы увидете все, как это бы выглядело и с помощью каких команд.
